{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa2f5b5",
   "metadata": {},
   "source": [
    "# RAG Implementation from Scratch\n",
    "\n",
    "In this project, I will create a simple RAG impelementation from scratch using Mistral model's chat completion model **mistral-large-latest**, and embedding model **mistral-embed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d12760a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"O3umMFr6EDLY78iAMnO1Ab1Ckl3KvwQn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8e3af",
   "metadata": {},
   "source": [
    "# Libraries Used\n",
    "\n",
    "**Requests Library**: We will use the requests library to fetch Paul Graham's essay data from the web. This library allows us to easily send HTTP requests and retrieve content, which will serve as the knowledge base for our Retrieval-Augmented Generation (RAG) application.\n",
    "\n",
    "**NumPy**: NumPy is the most popular Python library for numerical calculations and data manipulation. We will leverage it to perform efficient computations and handle data processing tasks required for preparing our essay data.\n",
    "\n",
    "**FAISS Vector Database**: We will utilize the FAISS (Facebook AI Similarity Search) vector database to store the vector embeddings generated from our data. This tool enables fast and scalable similarity searches, making it ideal for managing the embeddings used in our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a7188da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90edc0",
   "metadata": {},
   "source": [
    "## Data Fetching\n",
    "\n",
    "Read the essay data from the internet using **requests** and save it to file **essay.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15c4eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "684481d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('essay.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e314a",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) system, breaking a document into smaller pieces is important. This step which is called **chunking**, makes it easier to find and pull out the most relevant information later during the retrieval step. For this example, we split the text by characters, grouping every 2048 characters into a single chunk. Doing this gives us 37 chunks in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d519ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566884e",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "The next step is embedding, where we convert each text chunk into a numeric vector using Mistral AI’s mistral-embed model, enabling efficient retrieval in our RAG system.\n",
    "\n",
    "Embeddings are numeric vectors that represent text in a way computers can understand, placing similar meanings closer together in a virtual space. In a Retrieval-Augmented Generation (RAG) system, we need them to quickly find the most relevant text chunks by comparing their vectors to a question’s vector. This helps the system retrieve meaningful information efficiently and pass it to the AI for generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bcb1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "          model=\"mistral-embed\",\n",
    "          inputs=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f1a9c",
   "metadata": {},
   "source": [
    "Here, a timer is added to create a delay between requests, helping us stay within the Mistral API's request limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e45ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "text_embeddings = []\n",
    "for chunk in chunks:\n",
    "    text_embeddings.append(get_text_embedding(chunk))\n",
    "    time.sleep(2)  # Wait 1 second between requests; adjust as needed\n",
    "text_embeddings = np.array(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeea5ea",
   "metadata": {},
   "source": [
    "## Storing Document Embeddings\n",
    "\n",
    "Storing vector embeddings into vector databases a common practice to for efficient processing and retrieval. For this project we are using FAISS vector database as it is freely available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef397df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febcd44",
   "metadata": {},
   "source": [
    "## Create embeddings for a question\n",
    "\n",
    "Whenever users ask a question, we also need to create embeddings for this question using the same embedding models as before. This will enable as perform vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6830a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What were the two main things the author worked on before college?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddcebc",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "After creating and storing the embeddings in the vector database, we will retrieve the most relevant chunks by performing a similarity search using the question’s embedding. This search will identify the top k chunks, which are then combined with the user’s question to form a prompt for the LLM model (mistral-large-latest) to generate an accurate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "283d5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d320c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "Depending on the question and context, add explanation or example where necessary.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f57ef",
   "metadata": {},
   "source": [
    "## Generating Response\n",
    "\n",
    "We can now use the prompt in the Mistral models' chat completion function to generate responses. As seen in the example below, our simple RAG app is working well, but there's still significant room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a7d1d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Before college, the author worked on writing and programming. Specifically, they wrote short stories and worked on programming using an IBM 1401 computer.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
